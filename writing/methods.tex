\subsection{The Data and Preprocessing}
The data consisted of 3,000 online reviews, of which 2,400 were taken as a training set and 600 were taken as a test set.  These reviews were given without explanation of source, but a personal perusal of the reviews revealed that they cover a variety of reviewable things, from cell phones to movies to restaurants.  The data were preprocessed using a code kindly provided by the professors.  This code tokenized the reviews as well as removed stop words.  It then determined a vocabulary list based on words that had a frequency across the entire training set above a certain threshold.  I tried different values for for this threshold, between 3 and 6 inclusive, to determine a best value to use.  The code then converted each review into a bag-of-words representation based on the vocabulary list.  The length of the vocabularly list for the threshold values of 3, 4, 5, and 6 are respectively 812, 640, 541, and 462.  These are the initial lengths of the feature vectors; feature selection will shorten these.

\subsection{The Real Meat of this Work: Classifying}

\subsection{\Na\ Bayes in Detail}
\Na\ Bayes may be a simple classifier, but to me it is special because it was the first classifier I formally learned.  Before that, machine learning was nothing but magic to me, but after I learned \Na\ Bayes I saw how formal, understandable, and simple machine learning was.  \Na\ Bayes is named such because it makes use of Bayes' theorem and also makes the ``na\"ive'' assumption that all the features are independent.  For the specific application of sentiment analysis, counter-examples of the independence assumption are easily thought of: the existence of the word ``great'' in a review can have a very different classification interpretation depending on whether the word ``not'' occurs before it.  Despite this, \Na\ Bayes still enjoys prominence as a classifier because of its simplicity.

