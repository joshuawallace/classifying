\subsection{The Data and Preprocessing}
The data consisted of 3,000 online reviews, of which 2,400 were taken as a training set and 600 were taken as a test set.  These reviews were given without explanation of source, but a personal perusal of the reviews revealed that they cover a variety of reviewable things, from cell phones to movies to restaurants.  The data were preprocessed using a code kindly provided by the professors.  This code tokenized the reviews as well as removed stop words, with the help of the \texttt{nltk} Python library.  It then determined a vocabulary list based on words that had a frequency across the entire training set above a certain threshold.  I tried different values for for this threshold, between 3 and 6 inclusive, to determine a best value to use.  The code then converted each review into a bag-of-words representation based on the vocabulary list.  The length of the vocabularly list for the threshold values of 3, 4, 5, and 6 are respectively 812, 640, 541, and 462.  These are the initial lengths of the feature vectors; feature selection will shorten these.

\subsection{The Real Meat of this Work: Classifying}

This work makes use of classifiers from the \texttt{nltk} \cite{bird2009} and \texttt{sklearn} \cite{scikit-learn} Python packages.  I use  the following classifiers:
\begin{itemize}
\item NB from \texttt{nltk}, which employs a Bernoulli estimator
\item NB from \texttt{sklearn}, employing three different estimators (Gaussian, Bernoulli, and multinomial)
\item Random Forest (RF) with 150 trees and an entropy-based splitter
(I was particularly interested in this one since a recent astrophysics study \cite{ogle2017} used a RF classifier to classify 450,000 of a certain type of binary star and I need to do something very similar for my PhD research.)
\end{itemize}
This work put some emphasis on feature selection.  Several feature selection algorithms were used:
\begin{itemize}
\item A simple ``by-hand'' one, where I tried to cull the features myself
\item Variance threshold
\item Select K-best, using three different scoring functions (chi2, f\_classif, mutual\_info\_classif)
\item Recursive feature elimination, using a Support Vector Classifiers estimator
\end{itemize}
Each combination of classifier and feature selection was evaluated using on the 600 reviews withheld for testing, as discussed above and based on quantities such as accuracy (number of correct classifications over total number of reviews), precision, recall, and F$_1$ score.


\subsection{\Na\ Bayes in Detail}
\Na\ Bayes may be a simple classifier, but to me it is special because it was the first classifier I formally learned.  Before that, machine learning was nothing but magic to me, but after I learned \Na\ Bayes I saw how formal, understandable, and simple machine learning was.  \Na\ Bayes is named such because it makes use of Bayes' theorem and also makes the ``na\"ive'' assumption that all the features are independent.  For the specific application of sentiment analysis, counter-examples of the independence assumption are easily thought of: the existence of the word ``great'' in a review can have a very different classification interpretation depending on whether the word ``not'' occurs before it.  Despite this, \Na\ Bayes still enjoys prominence as a classifier because of its simplicity.

The following description of the \Na\ Bayes classifier greatly benefitted from review of \cite{wiki:nb}.  Wikipedia may not be the most reliable source, but the math speaks for itself.  For a feature vector representation of some input $\vec{x}$ and a set of classifications $C_{1..k}$, the classifier's job is to calculate $p(C_k | \vec{x})$ and from this to decide which $C_k$ is to be assigned as the classification for the input data. For the \Na\ Bayes classifier, Bayes' theorem is invoked,
\begin{equation}
\label{eqn:a}
p(C_k | \vec{x}) = \frac{ p( \vec{x} | C_k) p(C_k)}{p(\vec{x})}.
\end{equation}
Since the denominator $p(\vec{x})$ is independent of $C_k$, it does not matter in this analysis and is thus ignored.

Using the chain rule,
\begin{equation}
\label{eqn:t}
p(C_k,\vec{x}) = p(C_k, x_1, ..., x_n) = p(C_k) \prod^n_{i=1}p(x_i| C_k, x_1, ..., x_{i-1}).
\end{equation}
Note that $i=1$ in Equation~(\ref{eqn:t}) as it is written would give an $x_0$ in the product, a value that doesn't exist.  This should be interpreted as meaning no values of $x$ go into the conditional probability inside the product for $i=1$.  Here is where the assumption of mutual independence among features comes in.  To be precise, all that is needed to be assumed is {\it conditional independence} among the $x_i$ given $C_k$.  If this is the case, then we can write $p(x_i|C_k,x_1,...,x_{i-1}) = p(x_i|C_k)$.  In this case, Equation~(\ref{eqn:t}) becomes
\begin{equation}
p(C_k,\vec{x}) = p(C_k) \prod^n_{i=1}p(x_i| C_k) = p(C_k)p(x_1,...,x_n|C_k).
\end{equation}
This can be plugged in to Equation~(\ref{eqn:a}) to obtain
\begin{equation}
\label{eqn:b}
p(C_k | \vec{x}) \propto p(C_k) \prod^n_{i=1}p(x_i| C_k),
\end{equation}
where I have ignored the evidence term.  Thus, if one can establish $p(C_k)$ and all the $p(x_i|C_k)$, then it is possible to calculate probabilities for each $C_k$ for any given $\vec{x}$.  The typical approach is then to select the $C_k$ with the largest probability as the classification for the input data.


The question now becomes how to evaluate the various $p(x_i| C_k)$ from a given set of training data.  There are different models that can be used to fit the probabilities given the training data, but here I will focus on the Bernoulli distribution, since it lends itself well to how the data are expressed in this work (vectors of zeroes and ones, zero meaning the given feature is not present in the data and one meaning the given feature is present in the data) as well as the binary nature of the classification.  The Bernoulli distribution is defined entirely by the probability $p$ that outcome will be 1 (positive review); the probability that the outcome will be 0 (negative review) is $1-p$.  Thus, to fit the Bernoulli distribution to data all we need is to calculate $p$.  A simple way to do this is, for a particular feature (word) and a particular set of training data (reviews), to simply calculate $p(\textrm{word}|C_k)$, i.e., the fraction of reviews of a given sentiment that contain the word.  However, simply doing this makes your trained model particularly vulnerable to insufficient training data.  For instance, if in your training data you never see a certain word occur in a negative review, then from Equation~(\ref{eqn:b}) the probability that any review that contains this word is negative will be 0.  It will not always be accurate to have the probability of a given classification shot down to 0 based on a single word, so there should be some sort of ``correction'' to prevent this in the \Na\ Bayes.  One such correction is to use ``add-one smoothing''.  This correction takes the fraction described above used to calculate $p(\textrm{word}|C_k)$ and adds 1 to the numerator and a number $K$ to the denominator equal to the total number of classes (for our analysis, $K=2$).  This ensures that there will never be $p=0$ for any classification given a feature.